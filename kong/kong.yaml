_format_version: "3.0"

services:
  - name: lunar-backend
    host: host.docker.internal
    port: 5872
    protocol: http
    routes:
      - name: backend-api
        paths:
          - /api
        strip_path: false
        plugins:
          - name: key-auth
            config:
              key_names:
                - apikey
              key_in_header: true
              key_in_query: true
              hide_credentials: false

  - name: lunar-backend-admin
    host: host.docker.internal
    port: 5872
    protocol: http
    routes:
      - name: admin-api
        paths:
          - ~/admin(/api/.*)$
        strip_path: false
        preserve_host: false
        protocols:
          - http
          - https
        plugins:
          - name: request-transformer
            config:
              replace:
                uri: $(uri_captures[1])
      - name: admin-dashboard
        paths:
          - /admin
        strip_path: true
        preserve_host: false
        protocols:
          - http
          - https
      - name: landing-page
        paths:
          - /
        strip_path: false
        preserve_host: false
        protocols:
          - http
          - https
        plugins:
          - name: request-transformer
            config:
              replace:
                uri: /api/info

  - name: llm-service
    url: http://localhost:32000
    routes:
      - name: llm-proxy
        paths:
          - /llm
        strip_path: true
        preserve_host: false
        protocols:
          - http
          - https
        plugins:
          - name: cors
            config:
              origins:
                - "*"
              methods:
                - GET
                - POST
                - OPTIONS
              headers:
                - Accept
                - Content-Type
                - apikey
                - Authorization
              exposed_headers:
                - X-Auth-Token
              credentials: true
              max_age: 3600
          - name: key-auth
            config:
              key_names:
                - apikey
              key_in_header: true
              key_in_query: true
              hide_credentials: false
          - name: lunar-gateway
            config:
              backend_url: http://host.docker.internal:5872
          - name: ai-proxy
            config:
              route_type: llm/v1/chat
              auth:
                header_name: Authorization
                header_value: "Bearer ${OPENAI_API_KEY}"
              model:
                provider: openai
                name: gpt-5
                options:
                  temperature: 1.0
                  max_tokens: null  # Kong 3.9.1+ treats null correctly (no limit)
              max_request_body_size: 1048576  # 1MB limit for large requests (Dyad sends full codebase)

  - name: ollama-service
    url: ${OLLAMA_BACKEND_URL}/v1/chat/completions
    routes:
      - name: ollama-proxy
        paths:
          - /local-llm
        strip_path: true
        preserve_host: false
        protocols:
          - http
          - https
        plugins:
          - name: key-auth
            config:
              key_names:
                - apikey
              key_in_header: true
              key_in_query: true
              hide_credentials: false
          - name: response-transformer
            config:
              add:
                headers:
                  - X-Kong-LLM-Model:ollama/${OLLAMA_MODEL_NAME}
          - name: lunar-gateway
            config:
              backend_url: http://host.docker.internal:5872
